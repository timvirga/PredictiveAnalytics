---
title: "STAT6020_PredictiveAnalytics_Assignment3"
date: "17/10/2021"
subtitle: Prepared by Tim Virga
output: pdf_document
---

```{r, out.height="50%", out.width="50%"}
# Use these two lines to read the data and set the column names
gy <- read.table("Yield.dat")
names(gy) <- c("x", "y", "yield", "B", "Ca", "Cu", "Fe",
"K", "Mg", "Mn", "Na", "P", "Zn")
# create basic plot
plot(I((y-1)*48.8)~I((x-1)*12.2), data=gy, xlab="east (metres)", type="n",
ylab="north (metres)", xlim=c(-10,360), ylim=c(-10,360))
# add grid lines
abline(v=(1:28-1)*12.2, h=(1:8-1)*48.8, col="grey")
# add secondary axes
axis(side=3, at=(1:28-1)*12.2, labels=1:28, line=0, cex.axis=0.7, mgp=c(3,1,0))
mtext("x", side=3, line=1, cex=0.7, adj=1)
axis(side=4, at=(1:8-1)*48.8, labels=1:8, line=0, cex.axis=0.7, mgp=c(3,1,0))
mtext("y", side=4, line=1, cex=0.7, adj=1)
# plot the site locations
points(I((y-1)*48.8)~I((x-1)*12.2), gy, pch=19)
# plot observation 200 in red
points(I((y-1)*48.8)~I((x-1)*12.2), gy, pch=19, col="red", subset=200)
title(sub="Observation 200 shown in red.", col="red")
# remove observation 200 from the data
gy <- gy[-200,]

```

## Regression Tree
### Q1

```{r}
library(tree)

tree.gy = tree(yield ~ ., data=gy)
plot(tree.gy)
text(tree.gy, cex=.7)

```
```{r}
tree.gy
```

Here we observe the list of decision rules generated by the tree() model, noting that for each node we have visibility of the split condition, number of samples at that split, deviance and predicted yval at that split condition or terminal node. 

### Q2

We are applying the decision rules to observation #22, condition statements and outcomes below are specific to observation #22:\
1. Beginning from the root node, the first split condition Cu < 1.435 is met, moving our observation prediction to the left child node P < 11.5\
2. P < 11.5 is a split node, observation does not meet the split condition, therefore moving to the right child node Fe < 186\
3. Fe < 186 is a split node, observation does not meet the split condition, therefore moving to the right terminal node associated with a predicted yield prediction of 149.60 along with 26 other observations. 

### Q3 
The stopping criteria used in this tree is made up of yield prediction values determined by the tree() function algorithm as being the most computationally cost-effective based on modelling the observation data. Importantly the criteria considers an optimal degree of purity in pruning the tree to ensure accuracy not just for training data, but also future test data through means of not splitting the observations to completely pure nodes that result in overfitting.  

\newpage
### Q4

```{r}
tree.gy2 = tree(yield~B+I(1/Ca)+log(Cu)+log(Fe)+I(1/K)+I(1/Mg)+log(Mn)+Na+log(P)+log(Zn+1), data=gy)
plot(tree.gy2)
text(tree.gy2, cex=.7)
gy22 <- gy[22,]
predict(tree.gy2, newdata=gy22)

```

Comparing the transformed soil nutrients with the untransformed soil nutrients, we observe that the split condition values that determine the left or right split are lower in threshold as a result of the transformation, outliers in particular are most affected by this sort of transformation. It is generally unnecessary to transform variables for regression tree models because there is no distance consideration between features in regression, unlike classification. With that being said, regression trees are mostly insensitive to transformed predictors. For example, in the case of observation #22 the prediction remains unchanged at 149.60 yield for transformed soil nutrients and untransformed soil nutrients.    

\newpage
### Q5

```{r}
# log linear model
gy.loglm = lm(yield~B+I(1/Ca)+log(Cu)+log(Fe)+I(1/K)+I(1/Mg)+log(Mn)+Na+log(P)+log(Zn+1), data=gy)
#RSS calculation
lm_res = sum(residuals(gy.loglm)^2) 
gy_res = sum(residuals(tree.gy)^2)
gy_res2 = sum(residuals(tree.gy2)^2)
lm_res # linear model RSS
gy_res # untransformed predictors RSS
gy_res2 # transformed predictors RSS
```
We have calculated the RSS by summing and squaring residuals for the linear model, tree based model and tree based model with transformed variables. We observe the smallest RSS appointed to the tree based model with untransformed variables, noting a negative impact to the RSS for the soil nutrients variables having made no modifications to the model other than transforming the variables.

### Q6
```{r}
summary(tree.gy)
summary(gy.loglm)
```
As we can see by comparing summaries of our linear model and the untransformed tree model, the tree based model has performed variable selection as part of the function to produce a better model; constructing the tree with Cu, P, K, Fe, x, B and Ca only. Whereas our linear model without manual modification, incorporates all variables for the RSS calculation.

Furthermore, tree based models are advantageous for their easy interpretability, tree based models often more intuitively reflect human decision making processes. Despite the known limitations of tree based models adopting a 'greedy' optimisation approach, there are methods that can be employed to address this such as Bagging, Random Forest and Boosting. 

\newpage
### Q7 
```{r, out.height="50%", out.width="50%"}
par(mfrow=c(1,2)) 
tree.gy.seq = prune.tree(tree.gy) # pruning plot for single decision tree
tree.gycv = cv.tree(tree.gy) # pruning plot for cross-validated trees
plot(tree.gy.seq) 
plot(tree.gycv)
print(tree.gy.seq$size) # size of tree
print(tree.gy.seq$dev) # single decision tree deviance
print(tree.gycv$dev) # cv trees deviance
```
A tree should be pruned if we are interested in determining the optimal number of variables for our tree based model with bias-variance trade-off considered. By pruning the original untransformed predictor variables using prune.tree() we can expect result of an increase in deviance with the reducing size of the tree, converging eventually to a tree size of 1. Introducing cross-validation to a tree pruning process cv.tree() will calculate deviance across different samples of entire training data set over K folds to as a function of the cost complexity parameter k, leaving out 10% of the samples with each iteration. The result of this is that in our cv.tree() plot, we observe a higher rate of error in models with more variables (larger trees) because the variance between the k-folds averages out to a higher deviance.  

It's important to run cv.tree() multiple times given the random nature of the data that is selected for modelling as part of the cross-validation method. In this example by averaging the $\alpha$ penalty term of the lowest deviance size tree a few times, we can be confident that the pruned tree will result in a better performing model that addresses the bias-variance trade-off. We observe that the optimal tree size in the cv.tree() output is around 6 to 8 leaf nodes. 

### Q8
```{r, out.height="50%", out.width="50%"}
library(randomForest)

# random forest model with 100 trees
rf.gy = randomForest(yield ~.,data=gy, importance =TRUE, ntree=100)
rf.gy
plot(rf.gy)
```
```{r}
rf.gy.RSS <- sum((gy$yield - predict(rf.gy))^2) # RSS calculation for rf.gy 
rf.gy.RSS
gy_res # RSS from earlier tree

```
Comparing the RSS of Q5 single decision tree 25,167 to the RSS of our random forest model 39,773 is not a valid comparison. The single decision tree model is built once from the entire dataset that adopts a greedy optimisation method for split conditions that may result in a lower RSS however, at a cost of bias. The rf method in our example is modelling 100 trees with random variable selection at each split to determine the prediction and then averages the results (or 'votes') to determine the best model, addressing the bias-variance trade-off more effectively.  

\newpage
### Q9

```{r, out.height="50%", out.width="50%"}
importance(rf.gy) # importance of variables
varImpPlot(rf.gy)
```

By calling importance() we get a list of all predictors' respective %IncMSE and IncNodePurity for the random forest model. %IncMSE refers to the mean decrease in accuracy for the predictor variable when the variable is taken out of the model, a high %IncMSE such as I(1/Ca), log(Zn+1), log(P) and log(Cu) represent the most important variables to reduce error in the model. IncNodePurity measures the difference between RSS before and after the splits for each variable averaged over all trees; we observe log(Cu), log(Zn+1) and log(P) with the greatest RSS differences.

Comparing this information to the LASSO linear model which has plotted the most important variables by evaluation of the size of the penalty term applied for the respective coefficients to converge to 0, we follow the increasing penalty term for each variable to determine importance. We observe similarities in the optimal feature selection between models where LASSO reports: log(Cu), log(Zn+1), I(1/K) and log(P) as the most important predictors. 

Interestingly, the LASSO model reports I(1/K) as being a much stronger predictor than the random forest model, this tells us that in the case of the LASSO model, once the $\lambda$ value reaches a high enough value, that I(1/K) reveals itself as a strong predictor of the response variable. 

Additionally, our LASSO model reported log(Zn+1) and log(Cu) as the two most important variables, in our random forest model they remained important variables however, positioned 2nd and 3rd in the scale of %IncMSE, suggesting that although these variables are strong predictors of the response variable, they may also be subject to a higher impact to bias due to their large contribution to node purity. 